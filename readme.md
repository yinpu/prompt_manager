# Prompt Manager

A powerful prompt management and version control tool that helps you effectively organize, track, and reuse AI prompts.

[English](README.md) | [ä¸­æ–‡](README_CN.md)

## Core Concepts

### Project Structure

Prompt Manager uses a hierarchical structure to organize prompts:

```
Project > Prompt > Version
```

File system storage structure:
```
root/
  project/
    prompt/
      v0001/
        prompt.txt     # Prompt content
        outputs.json   # Model outputs
        meta.json      # Metadata
```

### Key Concepts

- **Project**: A collection of related prompts, used to organize prompts for different domains or tasks
- **Prompt**: A container for a single prompt, which can include multiple versions
- **Version**: A specific variant of a prompt, containing:
  - **Content**: The actual text sent to AI models, which can include variable placeholders (like `{name}`) for dynamic replacement
  - **Model Outputs**: Responses from different AI models to the same prompt, which can be simple text outputs or complex structures with metadata
  - **Metadata**: Additional information associated with the prompt version, such as language, tags, use cases, etc., for easy classification and filtering


Install from source:

```bash
git clone https://github.com/yinpu/prompt_manager.git
cd prompt_manager
pip install -e .
```

## Quick Start

### Basic Usage

```python
from prompt_manager import PromptManager

# Initialize the manager (specify storage path)
pm = PromptManager("./save")

# Get or create a prompt
prompt = pm.get_prompt("/project_name/prompt_name")

# Add a new version (automatically generate version number)
prompt.add_version(
    # content: Prompt content, can include variable placeholders like {name} for later replacement
    content="Hello {name}!",
    
    # model_outputs: Output results from different models
    model_outputs={
        # Simple format: directly provide model output text
        "gpt-4o": "Hello, Alice ðŸ‘‹",
        # Complex format: includes output text and related metadata
        "llama3": {"output": "Hello, Alice.", "meta": {"temp": 0.3}},
    },
    
    # meta: Version metadata, used to store additional information related to this version
    meta={"lang": "en"},
)

# Save changes
prompt.save()
```

### Using Custom Version Numbers

```python
prompt.add_version(
    content="Hello {name}!",
    model_outputs={"gpt-4o": "Hi Alice ðŸ‘‹"},
    meta={"lang": "en"},
    version="english-v1",  # Custom version number
)
prompt.save()
```

### Modifying Existing Versions

```python
# Update metadata
prompt.modify_version(
    "v0001",
    meta_update={"reviewed": True},
)

# Add new model output
prompt.add_model_output(
    "v0001",
    "claude-3",
    {"output": "Hello, Alice! Nice to meet you.", "meta": {"temperature": 0.7}}
)

prompt.save(overwrite_existing=True)
```

### Selecting Specific Versions

```python
# Set a specific version as the current version (latest)
current_version = prompt.select_version("english-v1")
print(f"Current version: {current_version.version}")
```

### Import and Export

```python
# Export prompt (including all versions)
export_file = prompt.export("./my_prompt.json")

# Import to a new location
new_prompt = pm.import_prompt(export_file, "/new_project/new_prompt_name")
```

## Advanced Features

### Traversing Versions

```python
# Get all versions
for version in prompt.versions:
    print(f"Version: {version.version}")
    print(f"Content: {version.content}")
    print(f"Number of model outputs: {len(version.model_outputs)}")
    print(f"Metadata: {version.meta}")
```

### Deleting Versions

```python
prompt.delete_version("v0001")
prompt.save()
```

### Project Management

```python
# List all projects
projects = pm.list_projects()

# Get a project
project = pm.get_project("project_name")

# List all prompts in the project
prompts = project.list_prompts()
```

## Data Models

### PromptVersion

A prompt version contains:
- `version`: Version identifier (such as "v0001" or custom name like "english-v1"), used to uniquely identify a prompt version
- `content`: Prompt content text, the actual text sent to AI models, can include variable placeholders (like `{name}`) for dynamic replacement
- `model_outputs`: Collection of output results from different models for this prompt, can store responses from multiple AI models for comparison
- `meta`: Custom metadata, used to store additional information related to the prompt (such as language, domain, use case, tags, etc.)
- `created_at`: Creation time, automatically records the timestamp when the version was created

### ModelOutput

A model output contains:
- `model_name`: Model name, identifies the AI model that generated this output (such as "gpt-4o", "llama3", etc.)
- `output`: Output text generated by the model, the actual response content from the AI model to the prompt
- `meta`: Metadata related to this output, including parameters used in the generation process (such as temperature, top_p, maximum length, etc.) and other relevant information

## Use Cases

- **Prompt Engineering R&D**: Track the iteration and improvement process of prompts
- **Multi-model Comparison**: Compare responses from different AI models to the same prompt
- **Version Control**: Save historical versions of prompts for easy reference and comparison
- **Team Collaboration**: Import/export functionality makes it easy to share prompts between team members

## System Requirements

- Python 3.9 or higher
